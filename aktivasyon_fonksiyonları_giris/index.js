/*
SÃ¼per kanka, o zaman Ã¶nce **hayatla baÄŸlantÄ±lÄ±**, sade bir ÅŸekilde teorik anlatÄ±mÄ± yapalÄ±m:
(Bu iÅŸin temelini anlamadan pratikte deÄŸiÅŸiklik yapmak kuru ezber olur.)

---

## ğŸ”Œ Aktivasyon FonksiyonlarÄ± Nedir?

Bir yapay sinir aÄŸÄ± katmanÄ±ndaki **nÃ¶ronlar**, girdilere karÅŸÄ±lÄ±k bazÄ± sayÄ±lar hesaplar (bunlara "aÄŸÄ±rlÄ±klÄ± toplam" denir).
Ama bu **Ã§Ä±ktÄ±lar her zaman iÅŸe yaramaz**. Ã‡Ã¼nkÃ¼:

* Negatif deÄŸerler olabilir
* Sonsuz bÃ¼yÃ¼klÃ¼kte sayÄ±lar olabilir
* DoÄŸrusal (linear) olabilir â†’ KarmaÅŸÄ±k ÅŸeyleri Ã¶ÄŸrenemez

Ä°ÅŸte bu yÃ¼zden araya **aktivasyon fonksiyonu** girer.

---

## ğŸ§  Hayattan Ã–rnekle AnlatÄ±m:

### ğŸ“Durum:

Bir arkadaÅŸÄ±na 5 farklÄ± teklif yapÄ±yorsun (film, gezi, yemek, oyun, vs).
Onun bunlara olan ilgisi + geÃ§miÅŸ deneyimleriyle bir karar alÄ±yor.

Åimdi...

* ArkadaÅŸÄ±nÄ±n **zihinsel kararÄ±** â†’ yapay sinir aÄŸÄ±na benzer.
* **Her teklif iÃ§in ayrÄ± etki** â†’ aÄŸÄ±rlÄ±klar (weights)
* SonuÃ§: "YapalÄ±m mÄ±, yapmayalÄ±m mÄ±?" â†’ **aktivasyon fonksiyonu** gibi davranÄ±r.

---

## ğŸ§ª Peki, Aktivasyon Ne Yapar?

| AmaÃ§                         | AÃ§Ä±klama                                                                  |
| ---------------------------- | ------------------------------------------------------------------------- |
| **Karar sÄ±nÄ±rÄ± koyar**       | â€œBu deÄŸer 0â€™Ä±n altÄ±ndaysa boÅŸ verâ€ der (ReLU gibi)                        |
| **Ä°htimali Ã¶lÃ§er**           | "Bunu %80 yaparÄ±z, %20 yapmayÄ±z" der (Sigmoid gibi)                       |
| **Pozitif/negatif yorumlar** | â€œBu durum -1 kadar olumsuz, +1 kadar olumluâ€ der (Tanh gibi)              |
| **HiÃ§ dokunmaz**             | â€œDirekt Ã§Ä±ktÄ±yÄ± verâ€ der (Linear gibi â€“ genellikle Ã§Ä±kÄ±ÅŸ katmanÄ±nda olur) |

---

## ğŸ” Aktivasyon FonksiyonlarÄ±na Teorik BakÄ±ÅŸ:

### 1. **ReLU (Rectified Linear Unit)**

* En Ã§ok kullanÄ±lan.
* Negatifleri 0 yapar, pozitifi aynen bÄ±rakÄ±r.
* **Hayat Ã¶rneÄŸi:** "CanÄ±m istemezse hiÃ§ yapmam, ama istersem tam yaparÄ±m."

```math
f(x) = max(0, x)
```

---

### 2. **Sigmoid**

* Ã‡Ä±ktÄ±yÄ± 0 ile 1 arasÄ±na sÄ±kÄ±ÅŸtÄ±rÄ±r.
* Genellikle sÄ±nÄ±flandÄ±rmada (binary) kullanÄ±lÄ±r.
* **Hayat Ã¶rneÄŸi:** "Bu fikre %85 katÄ±lÄ±yorum."

```math
f(x) = 1 / (1 + e^(-x))
```

---

### 3. **Tanh (Hyperbolic Tangent)**

* Ã‡Ä±ktÄ± -1 ile 1 arasÄ±nda olur.
* Negatif ve pozitif yorumlarÄ± birlikte verir.
* **Hayat Ã¶rneÄŸi:** "Bu durum -1 kadar olumsuz, 0.5 kadar olumlu" gibi dÃ¼ÅŸÃ¼n.

```math
f(x) = (e^x - e^-x) / (e^x + e^-x)
```

---

### 4. **Linear**

* Ã‡Ä±ktÄ±ya hiÃ§ dokunmaz.
* Genellikle **regresyon Ã§Ä±kÄ±ÅŸlarÄ±nda** kullanÄ±lÄ±r.
* **Hayat Ã¶rneÄŸi:** "Ne hissediyorsam onu sÃ¶ylÃ¼yorum."

```math
f(x) = x
```

---

## ğŸ§­ Hangi Durumda Hangisi?

| Senaryo                               | Aktivasyon              |
| ------------------------------------- | ----------------------- |
| GÃ¶rsel, metin tanÄ±ma, regresyon       | ReLU                    |
| OlasÄ±lÄ±k / sÄ±nÄ±flandÄ±rma (evet-hayÄ±r) | Sigmoid                 |
| Duygu gibi + ve - ayrÄ±mÄ± olan iÅŸler   | Tanh                    |
| SayÄ± tahmini / regresyon Ã§Ä±kÄ±ÅŸÄ±       | Linear (sadece Ã§Ä±kÄ±ÅŸta) |

---

## âš ï¸ Not:

HiÃ§ aktivasyon fonksiyonu koymazsan:

* BÃ¼tÃ¼n katmanlar **doÄŸrusal dÃ¶nÃ¼ÅŸÃ¼m** olur.
* Ne kadar katman eklersen ekle, **tek satÄ±rlÄ±k denklem** gibi Ã§alÄ±ÅŸÄ±r.
* Bu durumda sinir aÄŸÄ±n **"derin" deÄŸil, "dÃ¼z Ã§izgi" olur.**

---

EÄŸer istersen ÅŸimdi bu Ã¶ÄŸrendiklerimizi **uygulamaya** dÃ¶kelim:

> AynÄ± veriyle 3 model yapalÄ±m ve karÅŸÄ±laÅŸtÄ±ralÄ±m:
>
> * ReLU modeli
> * Sigmoid modeli
> * Tanh modeli

HazÄ±rsan kod kÄ±smÄ±na geÃ§elim mi kanka?

 */
